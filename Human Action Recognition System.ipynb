{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28d7f1d-3819-46be-8dc5-0f60f059bb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Detected classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'HammerThrow', 'Hammering', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpRope', 'JumpingJack', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n",
      "Total classes: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alok\\anaconda3\\envs\\CDAC_Project\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: har_video_dataset.npz\n",
      "X: (54991, 30, 33, 2) y: (54991,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# HUMAN ACTION RECOGNITION – VIDEO DATASET (UCF-style)\n",
    "# Videos -> MediaPipe Pose -> 30-frame Sequences -> CNN + BiLSTM\n",
    "# Modes: build -> train -> realtime\n",
    "# Author: Alok Rathour\n",
    "# ============================================================\n",
    "\n",
    "# -------------------- MODE --------------------\n",
    "# \"build\"    : Read videos using train.csv -> create har_video_dataset.npz\n",
    "# \"train\"    : Train CNN + BiLSTM on pose sequences\n",
    "# \"realtime\" : Run live webcam detection\n",
    "MODE = \"build\"   # change to: build / train / realtime\n",
    "\n",
    "# -------------------- PATHS --------------------\n",
    "ROOT = \".\"                # CDAC Project folder\n",
    "TRAIN_CSV = \"train.csv\"   # has: clip_name, clip_path, label\n",
    "DATA_PATH = \"har_video_dataset.npz\"\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "MAX_SEQS_PER_VIDEO = 10    # limit per video to keep dataset size manageable\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# -------------------- IMPORTS --------------------\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "# -------------------- BASIC SETUP --------------------\n",
    "NUM_JOINTS = 33\n",
    "CHECKPOINT_PATH = \"har_video_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "# -------------------- POSE (MEDIAPIPE) --------------------\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_detector = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
    "\n",
    "def extract_pose(frame):\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = pose_detector.process(rgb)\n",
    "    pts = []\n",
    "    if res.pose_landmarks:\n",
    "        for lm in res.pose_landmarks.landmark:\n",
    "            pts.append([lm.x, lm.y])\n",
    "    else:\n",
    "        for _ in range(NUM_JOINTS):\n",
    "            pts.append([0.0, 0.0])\n",
    "    return np.array(pts, dtype=np.float32)\n",
    "\n",
    "# -------------------- DATASET CLASS --------------------\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        x = torch.tensor(self.X[i], dtype=torch.float32).unsqueeze(1)\n",
    "        y = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    return list(d[\"X\"]), list(d[\"y\"]), list(d[\"class_names\"])\n",
    "\n",
    "# -------------------- MODEL (STABLE) --------------------\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, ncls):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1))\n",
    "        )\n",
    "        self.feature = 32 * (NUM_JOINTS//4) * 2\n",
    "        self.lstm = nn.LSTM(self.feature, 64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.5), nn.Linear(64, ncls)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B,T,C,H,W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        o,_ = self.lstm(x)\n",
    "        return self.head(o[:,-1,:])\n",
    "\n",
    "# -------------------- BUILD FROM VIDEOS --------------------\n",
    "def build_from_videos():\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "    class_names = sorted(df[\"label\"].unique())\n",
    "    label_map = {c:i for i,c in enumerate(class_names)}\n",
    "\n",
    "    print(\"Detected classes:\", class_names)\n",
    "    print(\"Total classes:\", len(class_names))\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        rel_path = row[\"clip_path\"].lstrip(\"/\")\n",
    "        video_path = os.path.join(ROOT, rel_path)\n",
    "        label = label_map[row[\"label\"]]\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        poses = []\n",
    "        while True:\n",
    "            ok, fr = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            poses.append(extract_pose(fr))\n",
    "        cap.release()\n",
    "\n",
    "        seq_count = 0\n",
    "        for i in range(0, len(poses) - SEQUENCE_LENGTH + 1, SEQUENCE_LENGTH):\n",
    "            X.append(np.array(poses[i:i+SEQUENCE_LENGTH]))\n",
    "            y.append(label)\n",
    "            seq_count += 1\n",
    "            if seq_count >= MAX_SEQS_PER_VIDEO:\n",
    "                break\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    np.savez(DATA_PATH, X=X, y=y, class_names=np.array(class_names))\n",
    "    print(\"Dataset created:\", DATA_PATH)\n",
    "    print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "\n",
    "# -------------------- TRAINING HELPERS --------------------\n",
    "def accuracy(p, y):\n",
    "    return (p.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def balanced_loader(ds, bs):\n",
    "    cnt = Counter(ds.y)\n",
    "    print(\"Class Dist:\", cnt)\n",
    "    w = {c:1.0/cnt[c] for c in cnt}\n",
    "    sw = [w[l] for l in ds.y]\n",
    "    sampler = WeightedRandomSampler(sw, len(sw), replacement=True)\n",
    "    return DataLoader(ds, batch_size=bs, sampler=sampler)\n",
    "\n",
    "# -------------------- RUN --------------------\n",
    "if MODE == \"build\":\n",
    "    build_from_videos()\n",
    "\n",
    "elif MODE == \"train\":\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(DATA_PATH)\n",
    "\n",
    "    X, y, class_names = load_npz(DATA_PATH)\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "    idx = np.random.permutation(len(X))\n",
    "    split = int(0.8 * len(X))\n",
    "    tr_idx, va_idx = idx[:split], idx[split:]\n",
    "\n",
    "    tr = HARDataset([X[i] for i in tr_idx], [y[i] for i in tr_idx])\n",
    "    va = HARDataset([X[i] for i in va_idx], [y[i] for i in va_idx])\n",
    "\n",
    "    model = CNN_BiLSTM(NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    tr_ld = balanced_loader(tr, BATCH_SIZE)\n",
    "    va_ld = DataLoader(va, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    best = 0.0\n",
    "    for e in range(EPOCHS):\n",
    "        model.train(); tl=ta=0\n",
    "        for xb, yb in tr_ld:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward(); optimizer.step()\n",
    "            tl += loss.item(); ta += accuracy(out, yb)\n",
    "        tl/=len(tr_ld); ta/=len(tr_ld)\n",
    "\n",
    "        model.eval(); vl=va=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_ld:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "                vl += criterion(out, yb).item()\n",
    "                va += accuracy(out, yb)\n",
    "        vl/=len(va_ld); va/=len(va_ld)\n",
    "\n",
    "        print(f\"E{e+1} TL{tl:.3f} TA{ta:.3f} | VL{vl:.3f} VA{va:.3f}\")\n",
    "        if va > best:\n",
    "            best = va\n",
    "            torch.save({\"model\": model.state_dict(), \"classes\": class_names}, CHECKPOINT_PATH)\n",
    "            print(\"Saved best\")\n",
    "\n",
    "elif MODE == \"realtime\":\n",
    "    ck = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    class_names = ck[\"classes\"]\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "    model = CNN_BiLSTM(NUM_CLASSES).to(DEVICE)\n",
    "    model.load_state_dict(ck[\"model\"]); model.eval()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    buf = deque(maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    while True:\n",
    "        ok, fr = cap.read()\n",
    "        if not ok: break\n",
    "        buf.append(extract_pose(fr))\n",
    "\n",
    "        txt = \"Collecting\"\n",
    "        if len(buf) == SEQUENCE_LENGTH:\n",
    "            x = torch.tensor(np.array(buf), dtype=torch.float32).unsqueeze(0).unsqueeze(2).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model(x)\n",
    "                p = torch.softmax(out, 1)\n",
    "                c, i = p.max(1)\n",
    "            txt = f\"{class_names[int(i.item())]} ({float(c.item()):.2f})\"\n",
    "\n",
    "        cv2.putText(fr, txt, (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow(\"HAR\", fr)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "    cap.release(); cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a766d5d-0807-4e50-868d-f2a92cc3a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "E1 TL3.478 TA0.132 | VL16.473 VA0.000\n",
      "E2 TL2.885 TA0.241 | VL19.984 VA0.000\n",
      "Saved best\n",
      "E3 TL2.550 TA0.325 | VL23.386 VA0.002\n",
      "Saved best\n",
      "E4 TL2.349 TA0.374 | VL26.041 VA0.002\n",
      "E5 TL2.160 TA0.420 | VL28.090 VA0.003\n",
      "Saved best\n",
      "E6 TL2.017 TA0.458 | VL28.873 VA0.002\n",
      "E7 TL1.896 TA0.490 | VL30.948 VA0.004\n",
      "Saved best\n",
      "E8 TL1.818 TA0.514 | VL31.837 VA0.001\n",
      "E9 TL1.741 TA0.532 | VL32.131 VA0.002\n",
      "E10 TL1.646 TA0.557 | VL34.428 VA0.001\n",
      "E11 TL1.572 TA0.577 | VL32.965 VA0.002\n",
      "E12 TL1.526 TA0.590 | VL33.038 VA0.002\n",
      "E13 TL1.464 TA0.606 | VL34.534 VA0.003\n",
      "E14 TL1.426 TA0.614 | VL35.147 VA0.003\n",
      "E15 TL1.369 TA0.627 | VL35.689 VA0.001\n",
      "E16 TL1.322 TA0.641 | VL36.279 VA0.001\n",
      "E17 TL1.290 TA0.653 | VL35.566 VA0.002\n",
      "E18 TL1.262 TA0.657 | VL36.250 VA0.002\n",
      "E19 TL1.226 TA0.668 | VL38.021 VA0.002\n",
      "E20 TL1.197 TA0.675 | VL39.487 VA0.001\n",
      "E21 TL1.157 TA0.687 | VL37.658 VA0.002\n",
      "E22 TL1.132 TA0.691 | VL39.156 VA0.001\n",
      "E23 TL1.102 TA0.701 | VL38.584 VA0.001\n",
      "E24 TL1.076 TA0.705 | VL39.434 VA0.001\n",
      "E25 TL1.056 TA0.712 | VL40.311 VA0.001\n",
      "E26 TL1.037 TA0.719 | VL40.549 VA0.001\n",
      "E27 TL1.023 TA0.722 | VL40.486 VA0.001\n",
      "E28 TL1.010 TA0.727 | VL41.717 VA0.001\n",
      "E29 TL0.991 TA0.731 | VL41.967 VA0.001\n",
      "E30 TL0.969 TA0.737 | VL41.847 VA0.001\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# HUMAN ACTION RECOGNITION – VIDEO DATASET (UCF-style)\n",
    "# Videos -> MediaPipe Pose -> 30-frame Sequences -> CNN + BiLSTM\n",
    "# Modes: build -> train -> realtime\n",
    "# Author: Alok Rathour\n",
    "# ============================================================\n",
    "\n",
    "# -------------------- MODE --------------------\n",
    "# \"build\"    : Read videos using train.csv -> create har_video_dataset.npz\n",
    "# \"train\"    : Train CNN + BiLSTM on pose sequences\n",
    "# \"realtime\" : Run live webcam detection\n",
    "MODE = \"train\"   # change to: build / train / realtime\n",
    "\n",
    "# -------------------- PATHS --------------------\n",
    "ROOT = \".\"                # CDAC Project folder\n",
    "TRAIN_CSV = \"train.csv\"   # has: clip_name, clip_path, label\n",
    "DATA_PATH = \"har_video_dataset.npz\"\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "MAX_SEQS_PER_VIDEO = 10    # limit per video to keep dataset size manageable\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# -------------------- IMPORTS --------------------\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "# -------------------- BASIC SETUP --------------------\n",
    "NUM_JOINTS = 33\n",
    "CHECKPOINT_PATH = \"har_video_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "# -------------------- POSE (MEDIAPIPE) --------------------\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_detector = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
    "\n",
    "def extract_pose(frame):\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = pose_detector.process(rgb)\n",
    "    pts = []\n",
    "    if res.pose_landmarks:\n",
    "        for lm in res.pose_landmarks.landmark:\n",
    "            pts.append([lm.x, lm.y])\n",
    "    else:\n",
    "        for _ in range(NUM_JOINTS):\n",
    "            pts.append([0.0, 0.0])\n",
    "    return np.array(pts, dtype=np.float32)\n",
    "\n",
    "# -------------------- DATASET CLASS --------------------\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        x = torch.tensor(self.X[i], dtype=torch.float32).unsqueeze(1)\n",
    "        y = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    return list(d[\"X\"]), list(d[\"y\"]), list(d[\"class_names\"])\n",
    "\n",
    "# -------------------- MODEL (STABLE) --------------------\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, ncls):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1))\n",
    "        )\n",
    "        self.feature = 32 * (NUM_JOINTS//4) * 2\n",
    "        self.lstm = nn.LSTM(self.feature, 64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.5), nn.Linear(64, ncls)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B,T,C,H,W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        o,_ = self.lstm(x)\n",
    "        return self.head(o[:,-1,:])\n",
    "\n",
    "# -------------------- BUILD FROM VIDEOS --------------------\n",
    "def build_from_videos():\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "    class_names = sorted(df[\"label\"].unique())\n",
    "    label_map = {c:i for i,c in enumerate(class_names)}\n",
    "\n",
    "    print(\"Detected classes:\", class_names)\n",
    "    print(\"Total classes:\", len(class_names))\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        rel_path = row[\"clip_path\"].lstrip(\"/\")\n",
    "        video_path = os.path.join(ROOT, rel_path)\n",
    "        label = label_map[row[\"label\"]]\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        poses = []\n",
    "        while True:\n",
    "            ok, fr = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            poses.append(extract_pose(fr))\n",
    "        cap.release()\n",
    "\n",
    "        seq_count = 0\n",
    "        for i in range(0, len(poses) - SEQUENCE_LENGTH + 1, SEQUENCE_LENGTH):\n",
    "            X.append(np.array(poses[i:i+SEQUENCE_LENGTH]))\n",
    "            y.append(label)\n",
    "            seq_count += 1\n",
    "            if seq_count >= MAX_SEQS_PER_VIDEO:\n",
    "                break\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    np.savez(DATA_PATH, X=X, y=y, class_names=np.array(class_names))\n",
    "    print(\"Dataset created:\", DATA_PATH)\n",
    "    print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "\n",
    "# -------------------- TRAINING HELPERS --------------------\n",
    "def accuracy(p, y):\n",
    "    return (p.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def balanced_loader(ds, bs):\n",
    "    cnt = Counter(ds.y)\n",
    "    print(\"Class Dist:\", cnt)\n",
    "    w = {c:1.0/cnt[c] for c in cnt}\n",
    "    sw = [w[l] for l in ds.y]\n",
    "    sampler = WeightedRandomSampler(sw, len(sw), replacement=True)\n",
    "    return DataLoader(ds, batch_size=bs, sampler=sampler)\n",
    "\n",
    "# -------------------- RUN --------------------\n",
    "if MODE == \"build\":\n",
    "    build_from_videos()\n",
    "\n",
    "elif MODE == \"train\":\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(DATA_PATH)\n",
    "\n",
    "    X, y, class_names = load_npz(DATA_PATH)\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "    idx = np.random.permutation(len(X))\n",
    "    split = int(0.8 * len(X))\n",
    "    tr_idx, va_idx = idx[:split], idx[split:]\n",
    "\n",
    "    tr = HARDataset([X[i] for i in tr_idx], [y[i] for i in tr_idx])\n",
    "    va = HARDataset([X[i] for i in va_idx], [y[i] for i in va_idx])\n",
    "\n",
    "    model = CNN_BiLSTM(NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    tr_ld = balanced_loader(tr, BATCH_SIZE)\n",
    "    va_ld = DataLoader(va, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    best = 0.0\n",
    "    for e in range(EPOCHS):\n",
    "        model.train(); tl=ta=0\n",
    "        for xb, yb in tr_ld:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward(); optimizer.step()\n",
    "            tl += loss.item(); ta += accuracy(out, yb)\n",
    "        tl/=len(tr_ld); ta/=len(tr_ld)\n",
    "\n",
    "        model.eval(); vl=va=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_ld:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "                vl += criterion(out, yb).item()\n",
    "                va += accuracy(out, yb)\n",
    "        vl/=len(va_ld); va/=len(va_ld)\n",
    "\n",
    "        print(f\"E{e+1} TL{tl:.3f} TA{ta:.3f} | VL{vl:.3f} VA{va:.3f}\")\n",
    "        if va > best:\n",
    "            best = va\n",
    "            torch.save({\"model\": model.state_dict(), \"classes\": class_names}, CHECKPOINT_PATH)\n",
    "            print(\"Saved best\")\n",
    "\n",
    "elif MODE == \"realtime\":\n",
    "    ck = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    class_names = ck[\"classes\"]\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "    model = CNN_BiLSTM(NUM_CLASSES).to(DEVICE)\n",
    "    model.load_state_dict(ck[\"model\"]); model.eval()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    buf = deque(maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    while True:\n",
    "        ok, fr = cap.read()\n",
    "        if not ok: break\n",
    "        buf.append(extract_pose(fr))\n",
    "\n",
    "        txt = \"Collecting\"\n",
    "        if len(buf) == SEQUENCE_LENGTH:\n",
    "            x = torch.tensor(np.array(buf), dtype=torch.float32).unsqueeze(0).unsqueeze(2).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model(x)\n",
    "                p = torch.softmax(out, 1)\n",
    "                c, i = p.max(1)\n",
    "            txt = f\"{class_names[int(i.item())]} ({float(c.item()):.2f})\"\n",
    "\n",
    "        cv2.putText(fr, txt, (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow(\"HAR\", fr)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "    cap.release(); cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc41071c-f6f6-444a-be49-f2cd48078dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alok\\anaconda3\\envs\\CDAC_Project\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# HUMAN ACTION RECOGNITION – VIDEO DATASET (UCF-style)\n",
    "# Videos -> MediaPipe Pose -> 30-frame Sequences -> CNN + BiLSTM\n",
    "# Modes: build -> train -> realtime\n",
    "# Author: Alok Rathour\n",
    "# ============================================================\n",
    "\n",
    "# -------------------- MODE --------------------\n",
    "# \"build\"    : Read videos using train.csv -> create har_video_dataset.npz\n",
    "# \"train\"    : Train CNN + BiLSTM on pose sequences\n",
    "# \"realtime\" : Run live webcam detection\n",
    "MODE = \"realtime\"   # change to: build / train / realtime\n",
    "\n",
    "# -------------------- PATHS --------------------\n",
    "ROOT = \".\"                # CDAC Project folder\n",
    "TRAIN_CSV = \"train.csv\"   # has: clip_name, clip_path, label\n",
    "DATA_PATH = \"har_video_dataset.npz\"\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "MAX_SEQS_PER_VIDEO = 10    # limit per video to keep dataset size manageable\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# -------------------- IMPORTS --------------------\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "# -------------------- BASIC SETUP --------------------\n",
    "NUM_JOINTS = 33\n",
    "CHECKPOINT_PATH = \"har_video_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on:\", DEVICE)\n",
    "\n",
    "# -------------------- POSE (MEDIAPIPE) --------------------\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_detector = mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
    "\n",
    "def extract_pose(frame):\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = pose_detector.process(rgb)\n",
    "    pts = []\n",
    "    if res.pose_landmarks:\n",
    "        for lm in res.pose_landmarks.landmark:\n",
    "            pts.append([lm.x, lm.y])\n",
    "    else:\n",
    "        for _ in range(NUM_JOINTS):\n",
    "            pts.append([0.0, 0.0])\n",
    "    return np.array(pts, dtype=np.float32)\n",
    "\n",
    "# -------------------- DATASET CLASS --------------------\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        x = torch.tensor(self.X[i], dtype=torch.float32).unsqueeze(1)\n",
    "        y = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def load_npz(path):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    return list(d[\"X\"]), list(d[\"y\"]), list(d[\"class_names\"])\n",
    "\n",
    "# -------------------- MODEL (STABLE) --------------------\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, ncls):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1)),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2,1))\n",
    "        )\n",
    "        self.feature = 32 * (NUM_JOINTS//4) * 2\n",
    "        self.lstm = nn.LSTM(self.feature, 64, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.Dropout(0.5), nn.Linear(64, ncls)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B,T,C,H,W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(B, T, -1)\n",
    "        o,_ = self.lstm(x)\n",
    "        return self.head(o[:,-1,:])\n",
    "\n",
    "# -------------------- BUILD FROM VIDEOS --------------------\n",
    "def build_from_videos():\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "    class_names = sorted(df[\"label\"].unique())\n",
    "    label_map = {c:i for i,c in enumerate(class_names)}\n",
    "\n",
    "    print(\"Detected classes:\", class_names)\n",
    "    print(\"Total classes:\", len(class_names))\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        rel_path = row[\"clip_path\"].lstrip(\"/\")\n",
    "        video_path = os.path.join(ROOT, rel_path)\n",
    "        label = label_map[row[\"label\"]]\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        poses = []\n",
    "        while True:\n",
    "            ok, fr = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            poses.append(extract_pose(fr))\n",
    "        cap.release()\n",
    "\n",
    "        seq_count = 0\n",
    "        for i in range(0, len(poses) - SEQUENCE_LENGTH + 1, SEQUENCE_LENGTH):\n",
    "            X.append(np.array(poses[i:i+SEQUENCE_LENGTH]))\n",
    "            y.append(label)\n",
    "            seq_count += 1\n",
    "            if seq_count >= MAX_SEQS_PER_VIDEO:\n",
    "                break\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    np.savez(DATA_PATH, X=X, y=y, class_names=np.array(class_names))\n",
    "    print(\"Dataset created:\", DATA_PATH)\n",
    "    print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "\n",
    "# -------------------- TRAINING HELPERS --------------------\n",
    "def accuracy(p, y):\n",
    "    return (p.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def balanced_loader(ds, bs):\n",
    "    cnt = Counter(ds.y)\n",
    "    print(\"Class Dist:\", cnt)\n",
    "    w = {c:1.0/cnt[c] for c in cnt}\n",
    "    sw = [w[l] for l in ds.y]\n",
    "    sampler = WeightedRandomSampler(sw, len(sw), replacement=True)\n",
    "    return DataLoader(ds, batch_size=bs, sampler=sampler)\n",
    "\n",
    "# -------------------- RUN --------------------\n",
    "if MODE == \"build\":\n",
    "    build_from_videos()\n",
    "\n",
    "elif MODE == \"train\":\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(DATA_PATH)\n",
    "\n",
    "    X, y, class_names = load_npz(DATA_PATH)\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "    idx = np.random.permutation(len(X))\n",
    "    split = int(0.8 * len(X))\n",
    "    tr_idx, va_idx = idx[:split], idx[split:]\n",
    "\n",
    "    tr = HARDataset([X[i] for i in tr_idx], [y[i] for i in tr_idx])\n",
    "    va = HARDataset([X[i] for i in va_idx], [y[i] for i in va_idx])\n",
    "\n",
    "    model = CNN_BiLSTM(NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    tr_ld = balanced_loader(tr, BATCH_SIZE)\n",
    "    va_ld = DataLoader(va, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    best = 0.0\n",
    "    for e in range(EPOCHS):\n",
    "        model.train(); tl=ta=0\n",
    "        for xb, yb in tr_ld:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward(); optimizer.step()\n",
    "            tl += loss.item(); ta += accuracy(out, yb)\n",
    "        tl/=len(tr_ld); ta/=len(tr_ld)\n",
    "\n",
    "        model.eval(); vl=va=0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_ld:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "                vl += criterion(out, yb).item()\n",
    "                va += accuracy(out, yb)\n",
    "        vl/=len(va_ld); va/=len(va_ld)\n",
    "\n",
    "        print(f\"E{e+1} TL{tl:.3f} TA{ta:.3f} | VL{vl:.3f} VA{va:.3f}\")\n",
    "        if va > best:\n",
    "            best = va\n",
    "            torch.save({\"model\": model.state_dict(), \"classes\": class_names}, CHECKPOINT_PATH)\n",
    "            print(\"Saved best\")\n",
    "\n",
    "elif MODE == \"realtime\":\n",
    "    ck = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    class_names = ck[\"classes\"]\n",
    "    NUM_CLASSES = len(class_names)\n",
    "\n",
    "    model = CNN_BiLSTM(NUM_CLASSES).to(DEVICE)\n",
    "    model.load_state_dict(ck[\"model\"]); model.eval()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    buf = deque(maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    while True:\n",
    "        ok, fr = cap.read()\n",
    "        if not ok: break\n",
    "        buf.append(extract_pose(fr))\n",
    "\n",
    "        txt = \"Collecting\"\n",
    "        if len(buf) == SEQUENCE_LENGTH:\n",
    "            x = torch.tensor(np.array(buf), dtype=torch.float32).unsqueeze(0).unsqueeze(2).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                out = model(x)\n",
    "                p = torch.softmax(out, 1)\n",
    "                c, i = p.max(1)\n",
    "            txt = f\"{class_names[int(i.item())]} ({float(c.item()):.2f})\"\n",
    "\n",
    "        cv2.putText(fr, txt, (20,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow(\"HAR\", fr)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "    cap.release(); cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6056b436-1ca4-4036-ab93-607a329db5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ts</th>\n",
       "      <th>action</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-22 09:10:25</td>\n",
       "      <td>BabyCrawling</td>\n",
       "      <td>0.965336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2026-01-22 09:10:26</td>\n",
       "      <td>BabyCrawling</td>\n",
       "      <td>0.905540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2026-01-22 09:10:27</td>\n",
       "      <td>BabyCrawling</td>\n",
       "      <td>0.943782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2026-01-22 09:10:44</td>\n",
       "      <td>PushUps</td>\n",
       "      <td>0.916348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2026-01-22 09:10:44</td>\n",
       "      <td>PushUps</td>\n",
       "      <td>0.809909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   ts        action  confidence\n",
       "0   1  2026-01-22 09:10:25  BabyCrawling    0.965336\n",
       "1   2  2026-01-22 09:10:26  BabyCrawling    0.905540\n",
       "2   3  2026-01-22 09:10:27  BabyCrawling    0.943782\n",
       "3   4  2026-01-22 09:10:44       PushUps    0.916348\n",
       "4   5  2026-01-22 09:10:44       PushUps    0.809909"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "con = sqlite3.connect(\"actions.db\")\n",
    "df = pd.read_sql(\"SELECT * FROM action_logs\", con)\n",
    "con.close()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2812ad-383a-4a09-9379-84fbd25da747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDAC_Project",
   "language": "python",
   "name": "cdac_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
